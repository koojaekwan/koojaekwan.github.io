



Efficient Estimation of Word Representations in Vector Space

ICLR 13



Abstract

- two novel model architectures for computing continous vector representations of words from very large data sets
  - CBOW
  - Skip-gram
- How measure Quality of these representations?
  - Word similarity task
  - compared to other NNs.
- higher accuracy, lower computational cost

- vectors provide SOTA performance for measuring syntatic and semantic word similarities



Introduction

- 많은 NLP 방법들이 단어를 하나의 작은 단위로 취급
  - (이런 방법은 단어 사이의 유사도를 판별하기 힘듬, 단지 단어사전 속 index 느낌임)
- 그래도 이 방법이 가지는 장점
  - simplicity
  - robustness
  - performance
    - 많은 양의 데이터 속에서 학습시킨 단순한 모델 > 적은 데이터로 학습시키는 복잡한 모델
- 가장 유명한 예시 : N-gram model



- 하지만 당연히 단순한 모델의 한계도 있음
  - Automatic speech recoginition
    - the amount of relevant in-domain data (그 사람이 말하는 어투같은걸 말하는듯) is limited
  - Machine Translation
    - existing corpora for many languages 데이터 수가 적음
  - **기존 모델들은 데이터셋의 크기에 성능에 영향을 많이 받음**

- 그래서 단순히 기존 테크닉을 scaling up하는 것보다, 좀 더 진화된 테크닉을 집중하는 것이 진전이 있음



- 최근의 머신러닝 기술의 발전과 함께 더욱 복잡한 모델을 매우 대량의 데이터 셋으로 학습 가능해졌고 그것은 일반적으로 성능면에서 간단한 모델을 능가한다.
- 아마도 가장 성공적인 컨셉은 단어의 분포를 표현[10]한 것이다. 예를 들어, 언어 모델을 기반으로 하는 신경망 구조는 N-gram을 상당히 능가한다. [1,27,17]





1.1 Goals of the Paper

- to introduce techniques that can be used for learning high-quality word vectors from huge datasets
  - 우리가 알기로는 어떤 이전의 논문들도 단어 벡터의 차원을 보통의 50- 100 차원으로하며, 수백만의 단어보다 많은 데이터를 이용해 성공적으로 훈련하지 못했다.

- vector representation quality 를 측정하기 위해
  - 기존 방식 Similar words
  - multiple degress of similarity도 측정
    - ex) Nouns can have multiple word endings

- 마치 수학 연산하듯이 similarity를 측정할 수 있음
  - vector("king") - vector("Man") + vector("Woman")
  - => vector("Queen")과 비슷하게 나옴



- 단어 사이 간의 선형적인 regularities를 보존하는 새로운 모델을 발전킴으로써, 이러한 벡터 연산들의 정확도를 최대화하려고 노력함
- syntactic and smemantic regularities를 측정했고 높은 정확도를 보임
- word dim과 데이터 양에 따라 성능과 학습시간이 얼마나인지 측정



1.2 Previous Work

- 단어들을 continous vector로 representation하는 연구는 긴 역사...
  - [10]
  - [26]
  - [8]

- NNLM 가장 유명함
  - [1]
  - 
  - ​	











2 Model Architectures

Latent Semantic Analysis (LSA)와 Latent Dirichlet Allocation (LDA)를 포함해, 단어들의 continous vector representation을 추정하기 위해

여러 모델들이 제시되었다.

여기서는 NN에 의해 학습되는 distributed representations에 초점을 맞출 거임

- linear regularities among words를 보존하는 LSA보다 훨씬 좋은 성능을 내기 때문에...[20, 31]

- 또한, LDA는 큰 데이터셋에 대해 컴퓨티에ㅣ셔널리 비쌈



모델마다 computational complexity를 먼저 정의 (as a number of parameters)

-> will try to maximize the accuracy, while minimizing the computational complexity



모든 모델들을 비교하기 위해, training complexity는 다음에 비례함

$$O = E \times T \times Q$$

$E$ (number of the training epochs)

$T$ (number of the words in the training set)

$Q$ (defined further for each model architeture)

일반적으로, $E$ 는 3~50, $T$ 는 one billion까지

모든 모델들은 SGD와 backpropagation으로 학습



Feedforward Neural Net Language Model (NNLM)

- [1]
- input, projection, hidden and output layers로 구성
- <img src="/Users/skcc10170/Library/Application Support/typora-user-images/image-20200417180558720.png" alt="image-20200417180558720" style="zoom:33%;" />
  - NNLM은 n-gram LM과 비슷하게, 앞에 모든 단어들을 참고하는 것이 아니라 일부 단어만 참고하는데
  - 이를 window라고 함
- input layer
  - $N$ 개의 이전 단어들은 $\text{1-of-}V$ 로 인코딩됨
    - 여기서 $V$ 는 vocabulary 크기를 말함(단어 사전)
- projection layer $P$
  - using dimensionality $N \times D$ (shared projection matrix)
  - 결국 위의 행렬은 lookup table과 같음





- Hideen layer
  - NNLM이 computationaly complex되는 부분은 between the projection and the hidden layer

  - as values in the projection layer are dense

  - 일반적으로 $N=10$ 으로 사용, $P$ 는 500에서 2000정도

    $H$ : hidden layer size (500에서 1000)

- output layer

  - dimensionality $V$
  - probability distributio 

이를 종합해, 계산 복잡도를 표시하면

- $Q = N \times D + N \times D \times H + H \times V$
- dominating term $H \times V$



이러한 복잡도를 피하기 위해, 다양한 방법들이 제안됨

- using Hierarchical versions
  - [25, 23, 18]
- avoiding normalized models complelety by using models that are not normalizied during training
  - [4, 9]
- vocabulary의 binary tree representations와 함께,
  output units는 약 $log_2{V}$ ㄱ까지 
- 따라서 복잡도의 대부분은 $N \times D \times H$ 에서 발생함



(이번 예제에서는 7개의 단어만 사용했지만, 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은 무엇일까요? NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게되는 것에 있습니다. 이렇게 되면 훈련이 끝난 후 다음 단어를 예측 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라고 하더라도 다음 단어를 선택할 수 있습니다. )





















https://wikidocs.net/45609

- NNLM

























